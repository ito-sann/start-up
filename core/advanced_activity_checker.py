"""
é«˜åº¦ç‰ˆ æ´»å‹•åˆ¤å®šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
- 1éšå±¤ä»¥ä¸Šã®ãƒšãƒ¼ã‚¸é·ç§»
- Peatix/connpass/Facebookã‚¤ãƒ™ãƒ³ãƒˆæ¤œçŸ¥
- å’Œæš¦ãƒ»ç›¸å¯¾è¡¨è¨˜ã®æ—¥ä»˜æ­£è¦åŒ–
- ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆã®æ§‹é€ åŒ–å‡ºåŠ›
"""

import re
import asyncio
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from urllib.parse import urljoin, urlparse
import sys
import os

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from playwright.async_api import async_playwright, Page
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False


# ç¾åœ¨ã®åŸºæº–æ—¥
CURRENT_DATE = datetime(2026, 2, 4)
THRESHOLD_DATE = CURRENT_DATE - timedelta(days=60)  # 2025-12-04

# æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå„ªå…ˆåº¦é †ï¼‰
DATE_PATTERNS = [
    # 2026å¹´2æœˆ4æ—¥, 2026å¹´02æœˆ04æ—¥
    (r'(\d{4})å¹´\s*(\d{1,2})æœˆ\s*(\d{1,2})æ—¥', 'ymd'),
    # 2026/2/4, 2026/02/04
    (r'(\d{4})/(\d{1,2})/(\d{1,2})', 'ymd'),
    # 2026-02-04
    (r'(\d{4})-(\d{2})-(\d{2})', 'ymd'),
    # 2026.02.04
    (r'(\d{4})\.(\d{1,2})\.(\d{1,2})', 'ymd'),
    # ä»¤å’Œ8å¹´2æœˆ4æ—¥
    (r'ä»¤å’Œ(\d{1,2})å¹´\s*(\d{1,2})æœˆ\s*(\d{1,2})æ—¥', 'reiwa'),
    # R8.2.4
    (r'R(\d{1,2})\.(\d{1,2})\.(\d{1,2})', 'reiwa'),
    # 2/4ï¼ˆä»Šå¹´ã¨ä»®å®šï¼‰
    (r'(\d{1,2})/(\d{1,2})(?!\d)', 'md'),
    # 2æœˆ4æ—¥
    (r'(\d{1,2})æœˆ\s*(\d{1,2})æ—¥', 'md'),
]

# ã‚¤ãƒ™ãƒ³ãƒˆãƒšãƒ¼ã‚¸ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
EVENT_KEYWORDS = [
    'event', 'events', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'ã‚»ãƒŸãƒŠãƒ¼', 'seminar',
    'news', 'ãŠçŸ¥ã‚‰ã›', 'ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æ–°ç€', 'topics',
    'calendar', 'ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼', 'schedule', 'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«',
    'report', 'æ´»å‹•å ±å‘Š', 'activity'
]

# å¤–éƒ¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®ãƒ‰ãƒ¡ã‚¤ãƒ³
EXTERNAL_PLATFORMS = {
    'peatix.com': 'Peatix',
    'connpass.com': 'connpass',
    'facebook.com/events': 'Facebook',
    'fb.me': 'Facebook',
    'eventbrite.com': 'Eventbrite',
    'doorkeeper.jp': 'Doorkeeper'
}


def parse_date_string(text: str) -> Optional[datetime]:
    """æ§˜ã€…ãªå½¢å¼ã®æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã«å¤‰æ›"""
    text = text.strip()
    
    for pattern, date_type in DATE_PATTERNS:
        match = re.search(pattern, text)
        if match:
            groups = match.groups()
            try:
                if date_type == 'ymd':
                    year, month, day = int(groups[0]), int(groups[1]), int(groups[2])
                elif date_type == 'reiwa':
                    # ä»¤å’Œå¤‰æ›ï¼ˆä»¤å’Œ1å¹´ = 2019å¹´ï¼‰
                    year = 2018 + int(groups[0])
                    month, day = int(groups[1]), int(groups[2])
                elif date_type == 'md':
                    # æœˆæ—¥ã®ã¿ã®å ´åˆã€ä»Šå¹´ã¾ãŸã¯æ¥å¹´ã¨ä»®å®š
                    month, day = int(groups[0]), int(groups[1])
                    year = CURRENT_DATE.year
                    # æœˆãŒç¾åœ¨ã‚ˆã‚Šå‰ãªã‚‰æ¥å¹´ã®å¯èƒ½æ€§
                    if month < CURRENT_DATE.month:
                        year += 1
                else:
                    continue
                
                if 1 <= month <= 12 and 1 <= day <= 31:
                    return datetime(year, month, day)
            except:
                continue
    
    return None


def extract_all_dates(text: str) -> List[Tuple[datetime, str]]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å…¨ã¦ã®æ—¥ä»˜ã¨ãã®å‘¨è¾ºæ–‡è„ˆã‚’æŠ½å‡º"""
    results = []
    lines = text.split('\n')
    
    for line in lines:
        for pattern, date_type in DATE_PATTERNS:
            for match in re.finditer(pattern, line):
                parsed = parse_date_string(match.group())
                if parsed and 2020 <= parsed.year <= 2030:
                    # å‘¨è¾ºã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«å€™è£œï¼‰ã‚’å–å¾—
                    context = line.strip()[:100]
                    results.append((parsed, context))
    
    # é‡è¤‡é™¤å»ã—ã¦æ—¥ä»˜é™é †ã§ã‚½ãƒ¼ãƒˆ
    seen = set()
    unique_results = []
    for date, context in sorted(results, key=lambda x: x[0], reverse=True):
        date_str = date.strftime('%Y-%m-%d')
        if date_str not in seen:
            seen.add(date_str)
            unique_results.append((date, context))
    
    return unique_results


class AdvancedActivityChecker:
    """é«˜åº¦ç‰ˆæ´»å‹•åˆ¤å®šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.current_date = CURRENT_DATE
        self.threshold_date = THRESHOLD_DATE
    
    async def find_event_links(self, page: Page, base_url: str) -> List[Dict]:
        """ã‚¤ãƒ™ãƒ³ãƒˆé–¢é€£ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ç´¢"""
        links = await page.evaluate("""
            Array.from(document.querySelectorAll('a')).map(a => ({
                text: a.innerText.trim().toLowerCase(),
                href: a.href,
                ariaLabel: a.getAttribute('aria-label') || ''
            })).filter(a => a.href && a.href.startsWith('http'))
        """)
        
        event_links = []
        external_links = []
        
        for link in links:
            href = link['href']
            text = link['text']
            
            # å¤–éƒ¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒã‚§ãƒƒã‚¯
            for domain, platform in EXTERNAL_PLATFORMS.items():
                if domain in href:
                    external_links.append({
                        'url': href,
                        'platform': platform,
                        'text': text
                    })
                    break
            
            # å†…éƒ¨ã‚¤ãƒ™ãƒ³ãƒˆãƒšãƒ¼ã‚¸ãƒã‚§ãƒƒã‚¯
            for keyword in EVENT_KEYWORDS:
                if keyword in text or keyword in href.lower():
                    if urlparse(href).netloc == urlparse(base_url).netloc:
                        event_links.append({
                            'url': href,
                            'text': text,
                            'type': 'internal'
                        })
                    break
        
        return event_links, external_links
    
    async def get_page_text(self, page: Page) -> str:
        """ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        await page.evaluate("""
            document.querySelectorAll('script, style, nav, footer, header').forEach(el => el.remove());
        """)
        return await page.evaluate("document.body.innerText")
    
    async def extract_events_from_page(self, page: Page, url: str) -> List[Dict]:
        """ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡º"""
        text = await self.get_page_text(page)
        dates_with_context = extract_all_dates(text)
        
        events = []
        for date, context in dates_with_context[:20]:  # æœ€æ–°20ä»¶ã¾ã§
            # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¨æ¸¬
            title = context.split('|')[0].split('ã€')[0].strip()[:50]
            if not title or len(title) < 3:
                title = f"ã‚¤ãƒ™ãƒ³ãƒˆ ({date.strftime('%Y-%m-%d')})"
            
            events.append({
                'title': title,
                'date': date.strftime('%Y-%m-%d'),
                'link': url
            })
        
        return events
    
    async def check_external_platform(self, page: Page, platform_url: str, platform_name: str) -> List[Dict]:
        """å¤–éƒ¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—"""
        try:
            await page.goto(platform_url, timeout=30000, wait_until='domcontentloaded')
            await asyncio.sleep(2)  # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¾…ã¡
            
            text = await self.get_page_text(page)
            dates_with_context = extract_all_dates(text)
            
            events = []
            for date, context in dates_with_context[:10]:
                events.append({
                    'title': context[:50] if context else f"{platform_name}ã‚¤ãƒ™ãƒ³ãƒˆ",
                    'date': date.strftime('%Y-%m-%d'),
                    'link': platform_url,
                    'platform': platform_name
                })
            
            return events
        except Exception as e:
            print(f"    âš  {platform_name}ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def check_facility(self, url: str, facility_name: str = "") -> Dict[str, Any]:
        """æ–½è¨­ã®æ´»å‹•çŠ¶æ³ã‚’è©³ç´°ã«åˆ¤å®š"""
        print(f"\nğŸ” èª¿æŸ»é–‹å§‹: {facility_name or url}")
        
        result = {
            "facility_name": facility_name,
            "url": url,
            "status": "dormant",
            "last_event_date": None,
            "event_list": [],
            "external_platforms": [],
            "checked_pages": []
        }
        
        if not PLAYWRIGHT_AVAILABLE:
            result["error"] = "PlaywrightãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"
            return result
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
            )
            page = await context.new_page()
            
            try:
                # 1. ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹
                print(f"  ğŸ“„ ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸: {url}")
                await page.goto(url, timeout=30000, wait_until='domcontentloaded')
                await asyncio.sleep(1)
                
                # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡º
                top_events = await self.extract_events_from_page(page, url)
                result["event_list"].extend(top_events)
                result["checked_pages"].append(url)
                
                # 2. ã‚¤ãƒ™ãƒ³ãƒˆãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã¨å¤–éƒ¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’æ¢ç´¢
                event_links, external_links = await self.find_event_links(page, url)
                result["external_platforms"] = [e['platform'] for e in external_links]
                
                # 3. å†…éƒ¨ã‚¤ãƒ™ãƒ³ãƒˆãƒšãƒ¼ã‚¸ã‚’1éšå±¤é·ç§»
                for link in event_links[:3]:  # æœ€å¤§3ãƒšãƒ¼ã‚¸
                    link_url = link['url']
                    if link_url in result["checked_pages"]:
                        continue
                    
                    print(f"  ğŸ“„ ã‚¤ãƒ™ãƒ³ãƒˆãƒšãƒ¼ã‚¸: {link_url[:60]}...")
                    try:
                        await page.goto(link_url, timeout=30000, wait_until='domcontentloaded')
                        await asyncio.sleep(1)
                        
                        page_events = await self.extract_events_from_page(page, link_url)
                        result["event_list"].extend(page_events)
                        result["checked_pages"].append(link_url)
                    except:
                        continue
                
                # 4. å¤–éƒ¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’ãƒã‚§ãƒƒã‚¯
                for ext_link in external_links[:2]:  # æœ€å¤§2ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
                    print(f"  ğŸ”— å¤–éƒ¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {ext_link['platform']}")
                    ext_events = await self.check_external_platform(
                        page, ext_link['url'], ext_link['platform']
                    )
                    result["event_list"].extend(ext_events)
                
            except Exception as e:
                print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {e}")
                result["error"] = str(e)
            finally:
                await browser.close()
        
        # 5. ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆã—ã€é‡è¤‡é™¤å»
        seen_dates = set()
        unique_events = []
        for event in sorted(result["event_list"], key=lambda x: x['date'], reverse=True):
            if event['date'] not in seen_dates:
                seen_dates.add(event['date'])
                unique_events.append(event)
        result["event_list"] = unique_events[:20]  # æœ€æ–°20ä»¶
        
        # 6. 2ãƒ¶æœˆãƒ«ãƒ¼ãƒ«ã®é©ç”¨
        if result["event_list"]:
            latest_date_str = result["event_list"][0]['date']
            result["last_event_date"] = latest_date_str
            
            latest_date = datetime.strptime(latest_date_str, '%Y-%m-%d')
            if latest_date >= self.threshold_date:
                result["status"] = "active"
                print(f"  âœ… ã‚¢ã‚¯ãƒ†ã‚£ãƒ– (æœ€æ–°: {latest_date_str})")
            else:
                print(f"  ğŸ’¤ ä¼‘çœ  (æœ€æ–°: {latest_date_str})")
        else:
            print(f"  â“ ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ãªã—")
        
        return result
    
    async def check_multiple_facilities(self, facilities: List[Dict]) -> List[Dict]:
        """è¤‡æ•°æ–½è¨­ã‚’ä¸€æ‹¬ãƒã‚§ãƒƒã‚¯"""
        results = []
        
        for facility in facilities:
            result = await self.check_facility(
                facility.get('website', facility.get('url', '')),
                facility.get('name', '')
            )
            result['facility_id'] = facility.get('id', '')
            results.append(result)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            await asyncio.sleep(2)
        
        return results


async def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    checker = AdvancedActivityChecker()
    
    # ãƒ†ã‚¹ãƒˆæ–½è¨­
    test_url = "https://tib.metro.tokyo.lg.jp/"
    result = await checker.check_facility(test_url, "Tokyo Innovation Base")
    
    print("\n" + "="*60)
    print("ğŸ“Š èª¿æŸ»çµæœ (JSON)")
    print("="*60)
    print(json.dumps(result, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    asyncio.run(main())
